{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "collate_data.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "i5WgbOwZDEfp",
        "-5RiXpKqPrV1",
        "yyx8tsfbTRSx",
        "djYaffGdTWH4",
        "KdHt_kA7TZ2S",
        "Q3ELRRyyTjqR",
        "DEAe07aYTmGj",
        "w_0rkOZdY1yu",
        "5HdMqw8JZijs"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hfTDYRUkmRM"
      },
      "source": [
        "# BrahmiNet Corpus: 110 language pairs mined from ILCI parallel corpus.\n",
        "\n",
        "# Hindi WikiData Transliteration Pairs - Hindi dataset (90k pairs)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGB4SeXzhW04"
      },
      "source": [
        "# Hindi WikiData Transliteration Pairs - Hindi dataset (90k pairs)\n",
        "# -remaining"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5WgbOwZDEfp"
      },
      "source": [
        "# Indic_Xlit_transliteration_parallel_corpora"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRpJIJI_ZoVC"
      },
      "source": [
        "# indic-Xlit transliteration corpora\n",
        "\n",
        "import os \n",
        "os.mkdir('Indic_Xlit_transliteration_parallel_corpora')\n",
        "\n",
        "# There are 12 languages represented in the dataset: \n",
        "# Bangla (bn), Gujarati (gu), Hindi (hi), Kannada (kn), Malayalam (ml), Marathi (mr), \n",
        "# Punjabi (pa), Sindhi (sd), Sinhala (si), Tamil (ta), Telugu (te) and Urdu (ur).\n",
        "# format of word pair native\\tromanized  \n",
        "\n",
        "lang_list = ['Bangla', 'Gujarati', 'Hindi', 'Kannada', 'Malayalam', 'Marathi', 'Punjabi', 'Sindhi', 'Sinhala', 'Tamil', 'Telugu', 'Urdu', 'Konkani', 'Maithili']\n",
        "lang_abr_list = ['bn', 'gu', 'hi', 'kn', 'ml', 'mr', 'pa', 'sd', 'si', 'ta', 'te', 'ur', 'gom', 'mai']\n",
        "\n",
        "# creating subdirectories and corresponding txt file\n",
        "for lang, lang_abr in zip(lang_list, lang_abr_list):\n",
        "    os.mkdir('Indic_Xlit_transliteration_parallel_corpora/'+lang)\n",
        "    f = open('Indic_Xlit_transliteration_parallel_corpora/'+lang+'/' + lang_abr + '_train.txt', \"a\")\n",
        "    f.close()\n",
        "    f = open('Indic_Xlit_transliteration_parallel_corpora/'+lang+'/' + lang_abr + '_dev.txt', \"a\")\n",
        "    f.close()\n",
        "    f = open('Indic_Xlit_transliteration_parallel_corpora/'+lang+'/' + lang_abr + '_test.txt', \"a\")\n",
        "    f.close()\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PWiAnyibNiA"
      },
      "source": [
        "# creating csv file to maintain counts of word pairs\n",
        "import numpy as np\n",
        "\n",
        "lang_index = {}\n",
        "for i, lang in enumerate(lang_list):\n",
        "    lang_index[lang] = i \n",
        "\n",
        "sampling_index = {'train' : 0, 'dev' : 1, 'test' : 2}\n",
        "\n",
        "dataset_index = {\n",
        "                \"Dakshina dataset\" : 0, \n",
        "                \"Xlit-Crowd\" : 1, \n",
        "                \"Xlit-IITB-Par\":2,\n",
        "                \"FIRE 2013 Track on Transliterated Search\":3, \n",
        "                \"AI4Bharat StoryWeaver Xlit Dataset\" : 4,\n",
        "                \"NotAI-tech English-Telugu\": 5,\n",
        "                \"Brahminet\" : 6,\n",
        "                \"NEWS 2018 Shared Task dataset\" : 7\n",
        "                }\n",
        "\n",
        "# intializing np array to maintain counts\n",
        "word_pairs_count = np.zeros(shape=(len(lang_list),len(dataset_index), 3), dtype=int)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5RiXpKqPrV1"
      },
      "source": [
        "# 0.Dakshina dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZzpW0rDJCLj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eebef9b4-4ee4-4738-cff4-749e46da3df9"
      },
      "source": [
        "# Dakshina Dataset: The Dakshina dataset is a collection of text in both Latin and native scripts for 12 South Asian languages. Contains an aggregate of around 300k word pairs and 120k sentence pairs.\n",
        "!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
        "import tarfile\n",
        "\n",
        "tar = tarfile.open('dakshina_dataset_v1.0.tar', \"r:\")\n",
        "tar.extractall()\n",
        "tar.close()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-16 11:07:06--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.141.128, 142.251.2.128, 2607:f8b0:4023:c0d::80, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.141.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G  39.8MB/s    in 19s     \n",
            "\n",
            "2021-07-16 11:07:25 (104 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9DCdY6QPv1u"
      },
      "source": [
        "# writing from lexicon subdir\n",
        "dataset = \"Dakshina dataset\"\n",
        "lang_abr_list = ['bn', 'gu', 'hi', 'kn', 'ml', 'mr', 'pa', 'sd', 'si', 'ta', 'te', 'ur']\n",
        "lang_list = ['Bangla', 'Gujarati', 'Hindi', 'Kannada', 'Malayalam', 'Marathi', 'Punjabi', 'Sindhi', 'Sinhala', 'Tamil', 'Telugu', 'Urdu']\n",
        "sampling = ['train', 'test', 'dev']\n",
        "\n",
        "for lang, lang_abr in zip(lang_list, lang_abr_list):\n",
        "    for file_type in sampling: \n",
        "        f_in = open('/content/dakshina_dataset_v1.0/'+lang_abr+'/lexicons/'+lang_abr+'.translit.sampled.'+file_type+'.tsv', \"r\", encoding=\"utf-8\")\n",
        "        lines = f_in.read().split(\"\\n\")\n",
        "\n",
        "        # taking first 2 word and joining with \\t\n",
        "        lines = [ \"\\t\".join( line.split(\"\\t\")[:-1] ) for line in lines ]\n",
        "\n",
        "        # removing empty strings\n",
        "        lexicons_lines = [line for line in lines if '\\t' in line]\n",
        "\n",
        "        # removing duplicates\n",
        "        lexicons_lines = list(set(lexicons_lines))\n",
        "\n",
        "        # counting word_pairs\n",
        "        word_pairs_count[lang_index[lang]][dataset_index[dataset]][sampling_index[file_type]] = len(lexicons_lines)\n",
        "\n",
        "        # writing in xlit corpora\n",
        "        f_out = open('Indic_Xlit_transliteration_parallel_corpora/'+lang+'/' + lang_abr + '_'+file_type+'.txt', \"a\")\n",
        "        f_out.write(\"\\n\".join(lexicons_lines))\n",
        "        f_out.write(\"\\n\")\n",
        "        \n",
        "        # closing objects\n",
        "        f_out.close()\n",
        "        f_in.close()\n",
        "        del lines\n",
        "        del lexicons_lines\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XEmWgO4g134"
      },
      "source": [
        "# # counting word pairs (romanized - lexicons)\n",
        "# lang_abr_list = ['bn', 'gu', 'hi', 'kn', 'ml', 'mr', 'pa', 'sd', 'si', 'ta', 'te', 'ur']\n",
        "# f_detail = open('stats_detail.txt','w+')\n",
        "# import re\n",
        "\n",
        "# for lang_abr in lang_abr_list:\n",
        "    \n",
        "#     f = open('/content/dakshina_dataset_v1.0/'+lang_abr+'/lexicons/'+lang_abr+'.translit.sampled.train.tsv', \"r\", encoding=\"utf-8\")\n",
        "#     lines = f.read().split(\"\\n\")\n",
        "#     lines = [ \"\\t\".join( line.split(\"\\t\")[:-1] ) for line in lines ]\n",
        "#     lines_lexicons = [line for line in lines if '\\t' in line]\n",
        "    \n",
        "#     f_detail.write(\"lexicons word pairs for \"+lang_abr+\" : \"+ str(len(lines_lexicons)))\n",
        "#     f_detail.write(\"\\n\")\n",
        "    \n",
        "#     lines_lexicons = list(set(lines_lexicons))\n",
        "#     f_detail.write(\"Unique lexicons word pairs for \"+lang_abr+\" : \"+ str(len(lines_lexicons)))\n",
        "#     f_detail.write(\"\\n\")\n",
        "\n",
        "\n",
        "#     f_update = open('lexicons_'+lang_abr+'.txt','w+')\n",
        "#     f_update.writelines('\\n'.join(lines_lexicons))\n",
        "#     f_update.close()\n",
        "#     f.close()\n",
        "\n",
        "#     f = open('/content/dakshina_dataset_v1.0/'+lang_abr+'/romanized/'+lang_abr+'.romanized.rejoined.aligned.tsv', \"r\", encoding=\"utf-8\")\n",
        "#     lines = f.read().split(\"\\n\")\n",
        "#     lines_romanized = [line for line in lines if '\\t' in line]\n",
        "#     f_detail.write(\"romanized word pairs for \"+lang_abr+\" : \"+ str(len(lines_romanized)))\n",
        "#     f_detail.write(\"\\n\")\n",
        "\n",
        "\n",
        "#     # f_update = open('romanized.txt','w+')\n",
        "#     # f_update.writelines('\\n'.join(lines_romanized))\n",
        "#     # f_update.close()\n",
        "\n",
        "#     lines_romanized = list(set(lines_romanized))\n",
        "#     f_detail.write(\"Unique romanized word pairs for \"+lang_abr+\" : \"+ str(len(lines_romanized)))\n",
        "#     f_detail.write(\"\\n\")\n",
        "\n",
        "#     # f_update = open('romanized_unique.txt','w+')\n",
        "#     # f_update.writelines('\\n'.join(lines_romanized))\n",
        "#     # f_update.close()\n",
        "#     # f.close()\n",
        "\n",
        "#     lines_romanized_set = set(lines_romanized)\n",
        "#     lines_lexicons_set = set(lines_lexicons)\n",
        "\n",
        "#     diff_set = list(lines_romanized_set.difference(lines_lexicons_set)) \n",
        "\n",
        "#     pattern = '[!@#$\\\")(\\'\\,%^&*?+:;{}<>/|\\[\\].`~-]'\n",
        "#     diff_set = [re.sub(pattern, '', line) for line in diff_set]\n",
        "\n",
        "#     # pattern = '[0-9]'\n",
        "#     # diff_set = [re.sub(pattern, '', line) for line in diff_set]\n",
        "\n",
        "#     diff_set = list(set(diff_set).difference(lines_lexicons_set)) \n",
        "#     diff_set = [line for line in diff_set if '\\t' in line]\n",
        "\n",
        "#     f_detail.write(\"New word pairs in romanized which are not in lexicons for \"+lang_abr+\" after removing special characters from romanized: \"+ str(len(diff_set)))\n",
        "#     f_detail.write(\"\\n\\n\\n\")\n",
        "\n",
        "\n",
        "#     f_update = open('romanized_minus_lexicon_'+lang_abr+'.txt','w+')\n",
        "#     f_update.writelines('\\n'.join(diff_set))\n",
        "#     f_update.close()\n",
        "#     f.close()\n",
        "\n",
        "# f_detail.close()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyx8tsfbTRSx"
      },
      "source": [
        "# 1.Xlit-Crowd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U1jwmkaJSPh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dce6565-47dd-47ce-b35a-54c86859a739"
      },
      "source": [
        "# Xlit-Crowd: Hindi-English Transliteration Corpus created via crowdsourcing. , 14919 pairs.\n",
        "# citation Mitesh M. Khapra, Ananthakrishnan Ramanathan, Anoop Kunchukuttan, Karthik Visweswariah, Pushpak Bhattacharyya. When Transliteration Met Crowdsourcing : An Empirical Study of Transliteration via Crowdsourcing using Efficient, Non-redundant and Fair Quality Control . Language and Resources and Evaluation Conference (LREC 2014). 2014.\n",
        "!wget https://raw.githubusercontent.com/anoopkunchukuttan/crowd-indic-transliteration-data/master/crowd_transliterations.hi-en.txt"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-16 11:08:17--  https://raw.githubusercontent.com/anoopkunchukuttan/crowd-indic-transliteration-data/master/crowd_transliterations.hi-en.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 380351 (371K) [text/plain]\n",
            "Saving to: ‘crowd_transliterations.hi-en.txt’\n",
            "\n",
            "crowd_transliterati 100%[===================>] 371.44K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-07-16 11:08:21 (13.5 MB/s) - ‘crowd_transliterations.hi-en.txt’ saved [380351/380351]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG0ce1PuUTuw"
      },
      "source": [
        "dataset = \"Xlit-Crowd\"\n",
        "lang = 'Hindi'\n",
        "lang_abr = 'hi'\n",
        "file_type = 'train'\n",
        "\n",
        "\n",
        "f_in = open('/content/crowd_transliterations.hi-en.txt', \"r\", encoding=\"utf-8\")\n",
        "lines = f_in.read().split(\"\\n\")\n",
        "\n",
        "# taking first 2 word and joining with \\t\n",
        "lines = [ \"\\t\".join( line.split(\"\\t\")[::-1] ) for line in lines ]\n",
        "\n",
        "# removing empty strings\n",
        "data_lines = [line for line in lines if '\\t' in line]\n",
        "\n",
        "# removing duplicates\n",
        "data_lines = list(set(data_lines))\n",
        "\n",
        "# counting word_pairs\n",
        "word_pairs_count[lang_index[lang]][dataset_index[dataset]][sampling_index[file_type]] = len(data_lines)\n",
        "\n",
        "\n",
        "# writing in xlit corpora\n",
        "f_out = open('Indic_Xlit_transliteration_parallel_corpora/'+lang+'/' + lang_abr + '_'+file_type+'.txt', \"a\")\n",
        "f_out.write(\"\\n\".join(data_lines))\n",
        "f_out.write(\"\\n\")\n",
        "\n",
        "# closing objects\n",
        "f_out.close()\n",
        "f_in.close()\n",
        "del lines\n",
        "del data_lines\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djYaffGdTWH4"
      },
      "source": [
        "# 2.Xlit-IITB-Par"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqR2cPnuMJ2n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c6de498-c500-449c-d718-e197eae2b48c"
      },
      "source": [
        "# Xlit-IITB-Par: Hindi-English Transliteration Corpus mined from parallel translation corpora.\n",
        "!wget http://www.cfilt.iitb.ac.in/iitb_parallel/supplementary_resources/xlit-iitb-par.tgz\n",
        "\n",
        "import tarfile\n",
        "tar = tarfile.open('xlit-iitb-par.tgz', 'r')\n",
        "for item in tar:\n",
        "    tar.extract(item, '/content/')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-16 11:08:21--  http://www.cfilt.iitb.ac.in/iitb_parallel/supplementary_resources/xlit-iitb-par.tgz\n",
            "Resolving www.cfilt.iitb.ac.in (www.cfilt.iitb.ac.in)... 103.21.127.134\n",
            "Connecting to www.cfilt.iitb.ac.in (www.cfilt.iitb.ac.in)|103.21.127.134|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.cfilt.iitb.ac.in/iitb_parallel/supplementary_resources/xlit-iitb-par.tgz [following]\n",
            "--2021-07-16 11:08:22--  https://www.cfilt.iitb.ac.in/iitb_parallel/supplementary_resources/xlit-iitb-par.tgz\n",
            "Connecting to www.cfilt.iitb.ac.in (www.cfilt.iitb.ac.in)|103.21.127.134|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.cfilt.iitb.ac.in/~parallelcorp/iitb_en_hi_parallel/supplementary_resources/xlit-iitb-par.tgz [following]\n",
            "--2021-07-16 11:08:23--  https://www.cfilt.iitb.ac.in/~parallelcorp/iitb_en_hi_parallel/supplementary_resources/xlit-iitb-par.tgz\n",
            "Reusing existing connection to www.cfilt.iitb.ac.in:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4884774 (4.7M) [application/x-gzip]\n",
            "Saving to: ‘xlit-iitb-par.tgz’\n",
            "\n",
            "xlit-iitb-par.tgz   100%[===================>]   4.66M  2.24MB/s    in 2.1s    \n",
            "\n",
            "2021-07-16 11:08:26 (2.24 MB/s) - ‘xlit-iitb-par.tgz’ saved [4884774/4884774]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLPmcwmkV-y2"
      },
      "source": [
        "import re\n",
        "# adding pairs with default threshold value\n",
        "dataset = \"Xlit-IITB-Par\"\n",
        "lang = 'Hindi'\n",
        "lang_abr = 'hi'\n",
        "file_type = 'train'\n",
        "\n",
        "\n",
        "f_in = open('/content/xlit-iitb-par/en-hi.mined-pairs', \"r\", encoding=\"utf-8\")\n",
        "lines = f_in.read().split(\"\\n\")\n",
        "\n",
        "# taking first 2 word and joining with \\t\n",
        "lines = [ \"\\t\".join( line.split(\"\\t\")[::-1] ) for line in lines ]\n",
        "\n",
        "# removing special characters\n",
        "pattern = '[!@#$\\\")(\\'\\,%^&*?+:;{}<>/|\\[\\].`~-]'\n",
        "lines = [re.sub(pattern, '', line) for line in lines]\n",
        "\n",
        "# removing empty strings\n",
        "data_lines = [line for line in lines if '\\t' in line]\n",
        "\n",
        "# removing duplicates\n",
        "data_lines = list(set(data_lines))\n",
        "\n",
        "# counting word_pairs\n",
        "word_pairs_count[lang_index[lang]][dataset_index[dataset]][sampling_index[file_type]] = len(data_lines)\n",
        "\n",
        "\n",
        "# writing in xlit corpora\n",
        "f_out = open('Indic_Xlit_transliteration_parallel_corpora/'+lang+'/' + lang_abr + '_'+file_type+'.txt', \"a\")\n",
        "f_out.write(\"\\n\".join(data_lines))\n",
        "f_out.write(\"\\n\")\n",
        "\n",
        "# closing objects\n",
        "f_out.close()\n",
        "f_in.close()\n",
        "del lines\n",
        "del data_lines"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdHt_kA7TZ2S"
      },
      "source": [
        "# 3.FIRE 2013 Track on Transliterated Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9t9FBKwZPOn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "795ca1bf-d781-411a-fcf0-ab11f65f13b0"
      },
      "source": [
        "# train data\n",
        "!wget https://akpublicdata.blob.core.windows.net/publicdata/transliteration_data/fire.zip\n",
        "!unzip fire.zip"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-16 11:08:26--  https://akpublicdata.blob.core.windows.net/publicdata/transliteration_data/fire.zip\n",
            "Resolving akpublicdata.blob.core.windows.net (akpublicdata.blob.core.windows.net)... 52.239.246.4\n",
            "Connecting to akpublicdata.blob.core.windows.net (akpublicdata.blob.core.windows.net)|52.239.246.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 904829 (884K) [application/x-zip-compressed]\n",
            "Saving to: ‘fire.zip’\n",
            "\n",
            "fire.zip            100%[===================>] 883.62K  2.31MB/s    in 0.4s    \n",
            "\n",
            "2021-07-16 11:08:27 (2.31 MB/s) - ‘fire.zip’ saved [904829/904829]\n",
            "\n",
            "Archive:  fire.zip\n",
            " extracting: fire/Bangla - Word transliteration pairs.zip  \n",
            "   creating: fire/Bangla_word_transliterations/\n",
            "   creating: fire/Bangla_word_transliterations/Chat/\n",
            "  inflating: fire/Bangla_word_transliterations/Chat/user-1.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Chat/user-10.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Chat/user-11.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Chat/user-12.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Chat/user-13.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Chat/user-14.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Chat/user-15.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Chat/user-16.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Chat/user-17.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Chat/user-18.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Chat/user-19.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Chat/user-2.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Chat/user-3.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Chat/user-4.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Chat/user-5.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Chat/user-6.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Chat/user-7.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Chat/user-8.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Chat/user-9.txt  \n",
            "   creating: fire/Bangla_word_transliterations/Dictation_Experiment/\n",
            "  inflating: fire/Bangla_word_transliterations/Dictation_Experiment/user-1.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Dictation_Experiment/user-10.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Dictation_Experiment/user-11.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Dictation_Experiment/user-12.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Dictation_Experiment/user-13.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Dictation_Experiment/user-14.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Dictation_Experiment/user-15.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Dictation_Experiment/user-16.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Dictation_Experiment/user-17.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Dictation_Experiment/user-18.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Dictation_Experiment/user-19.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Dictation_Experiment/user-2.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Dictation_Experiment/user-20.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Dictation_Experiment/user-3.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Dictation_Experiment/user-4.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Dictation_Experiment/user-5.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Dictation_Experiment/user-6.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Dictation_Experiment/user-7.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Dictation_Experiment/user-8.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Dictation_Experiment/user-9.txt  \n",
            "   creating: fire/Bangla_word_transliterations/Scenario/\n",
            "  inflating: fire/Bangla_word_transliterations/Scenario/user-1.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Scenario/user-10.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Scenario/user-11.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Scenario/user-12.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Scenario/user-13.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Scenario/user-14.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Scenario/user-15.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Scenario/user-16.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Scenario/user-17.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Scenario/user-18.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Scenario/user-2.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Scenario/user-3.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Scenario/user-4.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Scenario/user-5.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Scenario/user-6.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Scenario/user-7.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Scenario/user-8.txt  \n",
            "  inflating: fire/Bangla_word_transliterations/Scenario/user-9.txt  \n",
            "  inflating: fire/gujarati-pair.txt  \n",
            " extracting: fire/Hindi - Word transliteration pairs 2.zip  \n",
            "   creating: fire/Hindi_word_transliterations/\n",
            "   creating: fire/Hindi_word_transliterations/Chat/\n",
            "  inflating: fire/Hindi_word_transliterations/Chat/user-1.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Chat/user-10.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Chat/user-11.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Chat/user-12.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Chat/user-13.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Chat/user-14.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Chat/user-15.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Chat/user-16.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Chat/user-17.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Chat/user-18.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Chat/user-2.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Chat/user-3.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Chat/user-4.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Chat/user-5.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Chat/user-6.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Chat/user-7.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Chat/user-8.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Chat/user-9.txt  \n",
            "   creating: fire/Hindi_word_transliterations/Dictation_Experiment/\n",
            "  inflating: fire/Hindi_word_transliterations/Dictation_Experiment/user-1.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Dictation_Experiment/user-10.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Dictation_Experiment/user-11.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Dictation_Experiment/user-12.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Dictation_Experiment/user-13.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Dictation_Experiment/user-14.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Dictation_Experiment/user-15.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Dictation_Experiment/user-16.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Dictation_Experiment/user-17.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Dictation_Experiment/user-18.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Dictation_Experiment/user-2.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Dictation_Experiment/user-3.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Dictation_Experiment/user-4.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Dictation_Experiment/user-5.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Dictation_Experiment/user-6.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Dictation_Experiment/user-7.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Dictation_Experiment/user-8.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Dictation_Experiment/user-9.txt  \n",
            "   creating: fire/Hindi_word_transliterations/Scenario/\n",
            "  inflating: fire/Hindi_word_transliterations/Scenario/user-1.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Scenario/user-10.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Scenario/user-11.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Scenario/user-12.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Scenario/user-13.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Scenario/user-14.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Scenario/user-15.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Scenario/user-16.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Scenario/user-17.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Scenario/user-18.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Scenario/user-2.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Scenario/user-3.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Scenario/user-4.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Scenario/user-5.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Scenario/user-6.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Scenario/user-7.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Scenario/user-8.txt  \n",
            "  inflating: fire/Hindi_word_transliterations/Scenario/user-9.txt  \n",
            "  inflating: fire/hindi-pair.txt     \n",
            "  inflating: fire/index.html         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kE1ojieZeT2"
      },
      "source": [
        "import os\n",
        "import re\n",
        "dataset = \"FIRE 2013 Track on Transliterated Search\"\n",
        "\n",
        "lang_list = [\"Hindi\", \"Bangla\"]\n",
        "lang_abr_in_list = [\"Hindi\", \"Bangla\"]\n",
        "lang_abr_out_list = [\"hi\", \"bn\"]\n",
        "file_out_type = \"train\"\n",
        "\n",
        "for lang, lang_abr_in, lang_abr_out  in zip(lang_list, lang_abr_in_list, lang_abr_out_list):\n",
        "    dir_list = os.listdir('/content/fire/'+lang_abr_in+'_word_transliterations')\n",
        "    temp = []\n",
        "    for dir in dir_list:\n",
        "        file_list = os.listdir('/content/fire/'+lang_abr_in+'_word_transliterations/'+dir)\n",
        "\n",
        "        for f_in_name in file_list:\n",
        "            f_in = open('/content/fire/'+lang_abr_in+'_word_transliterations/'+dir+'/'+f_in_name)\n",
        "            lines = f_in.read().split('\\n')\n",
        "            lines = [line for line in lines if ';' in line]\n",
        "            native_lines = []\n",
        "            romanized_lines = []\n",
        "            \n",
        "            \n",
        "            for i, line in enumerate(lines):\n",
        "                if i%2==0:\n",
        "                    romanized_lines.append(line)\n",
        "                else:\n",
        "                    native_lines.append(line)\n",
        "            for native_sen, romanized_sen in zip(native_lines, romanized_lines):\n",
        "                native_words = native_sen.split(';')\n",
        "                romanized_words = romanized_sen.split(';')\n",
        "                for nat,rom in zip(native_words, romanized_words):\n",
        "                    temp.append(nat+\"\\t\"+rom)\n",
        "\n",
        "            f_in.close()       \n",
        "    lines = temp\n",
        "\n",
        "    # remove special characters\n",
        "    pattern = '[!@#$\\\"-_)।’‘“ ”–(\\'\\,%^&*?+:;{}<>/|\\[\\].`~-]'\n",
        "    lines = [re.sub(pattern, '', line) for line in lines]\n",
        "    \n",
        "    # remove duplicates\n",
        "    lines = list(set(lines))\n",
        "\n",
        "    # removing empty lines\n",
        "    lines = [line for line in lines if '' not in line.split('\\t') ]\n",
        "    \n",
        "    f_out = open('Indic_Xlit_transliteration_parallel_corpora/'+lang+'/' + lang_abr_out + '_'+file_out_type+'.txt', \"a\")\n",
        "    f_out.write('\\n'.join(lines))\n",
        "    f_out.write(\"\\n\")\n",
        "    f_out.close()\n",
        "\n",
        "    f_out = open('temp'+lang_abr_in+'.txt', \"w\")\n",
        "    f_out.write('\\n'.join(lines))\n",
        "    f_out.close()\n",
        "    \n",
        "    # counting word_pairs\n",
        "    word_pairs_count[lang_index[lang]][dataset_index[dataset]][sampling_index[file_out_type]] = len(lines)\n",
        "\n",
        "f_in_folder = open('tempHindi.txt', \"r\")\n",
        "lines_folder = f_in_folder.read().split('\\n')\n",
        "\n",
        "f_in_file = open('/content/fire/hindi-pair.txt', \"r\")\n",
        "lines_file = f_in_file.read().split('\\n')\n",
        "lines_file = ['\\t'.join(line.split('\\t')[::-1]) for line in lines_file ]\n",
        "\n",
        "diff_set = set(lines_file).difference(set(lines_folder))\n",
        "\n",
        "lang = \"Hindi\"\n",
        "lang_abr_out = \"hi\"\n",
        "file_out_type = \"train\"\n",
        "f_out = open('Indic_Xlit_transliteration_parallel_corpora/'+lang+'/' + lang_abr_out + '_'+file_out_type+'.txt', \"a\")\n",
        "f_out.write('\\n'.join(list(diff_set)))\n",
        "f_out.write(\"\\n\")\n",
        "f_out.close()\n",
        "\n",
        "# counting word_pairs\n",
        "word_pairs_count[lang_index[lang]][dataset_index[dataset]][sampling_index[file_out_type]] += len(list(diff_set))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB8BRaDKFLGh"
      },
      "source": [
        "# writing gujarati files\n",
        "dataset = \"FIRE 2013 Track on Transliterated Search\"\n",
        "\n",
        "f_in_file = open('/content/fire/gujarati-pair.txt', \"r\")\n",
        "lines_file = f_in_file.read().split('\\n')\n",
        "lines_file = ['\\t'.join(line.split('\\t')[::-1]) for line in lines_file ]\n",
        "lines_file = list(set(lines_file))\n",
        "lines_file = [line for line in lines_file if line]\n",
        "\n",
        "lang = \"Gujarati\"\n",
        "lang_abr_out = \"gu\"\n",
        "file_out_type = \"train\"\n",
        "f_out = open('Indic_Xlit_transliteration_parallel_corpora/'+lang+'/' + lang_abr_out + '_'+file_out_type+'.txt', \"a\")\n",
        "f_out.write('\\n'.join(lines_file))\n",
        "f_out.write(\"\\n\")\n",
        "f_out.close()\n",
        "\n",
        "# counting word_pairs\n",
        "word_pairs_count[lang_index[lang]][dataset_index[dataset]][sampling_index[file_out_type]] += len(lines_file)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3ELRRyyTjqR"
      },
      "source": [
        "# 4.AI4Bharat StoryWeaver Xlit Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f29fhHLklgQZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72e80363-07b9-451f-f163-6b8e2755f654"
      },
      "source": [
        "import zipfile\n",
        "# AI4Bharat StoryWeaver Xlit Dataset - Transliteration datasets for Hindi, Maithili & Konkani\n",
        "# Hindi\n",
        "!wget https://github.com/AI4Bharat/IndianNLP-Transliteration/releases/download/DATA/Hindi_Xlit_dataset.zip\n",
        "with zipfile.ZipFile('/content/Hindi_Xlit_dataset.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')\n",
        "# Konkani(Goan)\n",
        "!wget https://github.com/AI4Bharat/IndianNLP-Transliteration/releases/download/DATA/Konkani_Xlit_dataset.zip\n",
        "with zipfile.ZipFile('/content/Konkani_Xlit_dataset.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')\n",
        "\n",
        "# Maithili\n",
        "!wget https://github.com/AI4Bharat/IndianNLP-Transliteration/releases/download/DATA/Maithili_Xlit_dataset.zip\n",
        "with zipfile.ZipFile('/content/Maithili_Xlit_dataset.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-16 11:08:28--  https://github.com/AI4Bharat/IndianNLP-Transliteration/releases/download/DATA/Hindi_Xlit_dataset.zip\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-releases.githubusercontent.com/231321785/14c95280-01a2-11eb-921f-4221081fa4b2?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210716%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210716T110828Z&X-Amz-Expires=300&X-Amz-Signature=4bedd443eec6bc951bc1946a2058c911d3cc2fa00bcc285b65f8c41a45273560&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=231321785&response-content-disposition=attachment%3B%20filename%3DHindi_Xlit_dataset.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-07-16 11:08:28--  https://github-releases.githubusercontent.com/231321785/14c95280-01a2-11eb-921f-4221081fa4b2?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210716%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210716T110828Z&X-Amz-Expires=300&X-Amz-Signature=4bedd443eec6bc951bc1946a2058c911d3cc2fa00bcc285b65f8c41a45273560&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=231321785&response-content-disposition=attachment%3B%20filename%3DHindi_Xlit_dataset.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.108.154, 185.199.109.154, 185.199.110.154, ...\n",
            "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.108.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 609266 (595K) [application/octet-stream]\n",
            "Saving to: ‘Hindi_Xlit_dataset.zip’\n",
            "\n",
            "Hindi_Xlit_dataset. 100%[===================>] 594.99K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-07-16 11:08:28 (20.0 MB/s) - ‘Hindi_Xlit_dataset.zip’ saved [609266/609266]\n",
            "\n",
            "--2021-07-16 11:08:28--  https://github.com/AI4Bharat/IndianNLP-Transliteration/releases/download/DATA/Konkani_Xlit_dataset.zip\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-releases.githubusercontent.com/231321785/290d4f80-01a2-11eb-8fa8-ce7bbafb9299?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210716%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210716T110829Z&X-Amz-Expires=300&X-Amz-Signature=27d7504aaa6953b13445b2ae04debe934c73051ecbdf5ec8cd14b37f4a5a93c3&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=231321785&response-content-disposition=attachment%3B%20filename%3DKonkani_Xlit_dataset.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-07-16 11:08:29--  https://github-releases.githubusercontent.com/231321785/290d4f80-01a2-11eb-8fa8-ce7bbafb9299?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210716%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210716T110829Z&X-Amz-Expires=300&X-Amz-Signature=27d7504aaa6953b13445b2ae04debe934c73051ecbdf5ec8cd14b37f4a5a93c3&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=231321785&response-content-disposition=attachment%3B%20filename%3DKonkani_Xlit_dataset.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.108.154, 185.199.109.154, 185.199.110.154, ...\n",
            "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.108.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 375706 (367K) [application/octet-stream]\n",
            "Saving to: ‘Konkani_Xlit_dataset.zip’\n",
            "\n",
            "Konkani_Xlit_datase 100%[===================>] 366.90K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-07-16 11:08:29 (14.4 MB/s) - ‘Konkani_Xlit_dataset.zip’ saved [375706/375706]\n",
            "\n",
            "--2021-07-16 11:08:29--  https://github.com/AI4Bharat/IndianNLP-Transliteration/releases/download/DATA/Maithili_Xlit_dataset.zip\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-releases.githubusercontent.com/231321785/39bdc580-01a2-11eb-837f-f76186cb1b58?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210716%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210716T110829Z&X-Amz-Expires=300&X-Amz-Signature=0da6b29827f108b879549c7d0e53bcc144ad0b997c884f34ca5dc54f95b0b579&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=231321785&response-content-disposition=attachment%3B%20filename%3DMaithili_Xlit_dataset.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-07-16 11:08:29--  https://github-releases.githubusercontent.com/231321785/39bdc580-01a2-11eb-837f-f76186cb1b58?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210716%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210716T110829Z&X-Amz-Expires=300&X-Amz-Signature=0da6b29827f108b879549c7d0e53bcc144ad0b997c884f34ca5dc54f95b0b579&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=231321785&response-content-disposition=attachment%3B%20filename%3DMaithili_Xlit_dataset.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.108.154, 185.199.109.154, 185.199.110.154, ...\n",
            "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.108.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 590139 (576K) [application/octet-stream]\n",
            "Saving to: ‘Maithili_Xlit_dataset.zip’\n",
            "\n",
            "Maithili_Xlit_datas 100%[===================>] 576.31K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-07-16 11:08:29 (20.1 MB/s) - ‘Maithili_Xlit_dataset.zip’ saved [590139/590139]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykqMvTf44vbX"
      },
      "source": [
        "import json\n",
        "dataset = \"AI4Bharat StoryWeaver Xlit Dataset\"\n",
        "lang_list = [\"Hindi\", \"Konkani\", \"Maithili\"]\n",
        "lang_abr_in_list = ['Hi', 'Gom', 'Mai']\n",
        "lang_abr_out_list = ['hi', 'gom', 'mai']\n",
        "sampling_in = ['train', 'valid', 'test']\n",
        "sampling_out = ['train', 'dev', 'test']\n",
        "\n",
        "\n",
        "\n",
        "for lang, lang_abr_in, lang_abr_out  in zip(lang_list, lang_abr_in_list, lang_abr_out_list):\n",
        "    for file_in_type, file_out_type in zip(sampling_in, sampling_out):\n",
        "        \n",
        "        lines = []\n",
        "\n",
        "        # Opening JSON file\n",
        "        f_in = open('/content/'+lang_abr_in+'En_ann1_'+file_in_type+'.json')\n",
        "        data = json.load(f_in)\n",
        "\n",
        "        \n",
        "        for item in data:\n",
        "            nat = item\n",
        "            for rom in data[item]:\n",
        "                lines.append(nat+\"\\t\"+rom)\n",
        "        \n",
        "        # counting word_pairs\n",
        "        word_pairs_count[lang_index[lang]][dataset_index[dataset]][sampling_index[file_out_type]] = len(lines)\n",
        "\n",
        "        f_out = open('Indic_Xlit_transliteration_parallel_corpora/'+lang+'/' + lang_abr_out + '_'+file_out_type+'.txt', \"a\")\n",
        "        f_out.write(\"\\n\".join(lines))\n",
        "        f_out.write(\"\\n\")\n",
        "        f_out.close()\n",
        "        f_in.close()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEAe07aYTmGj"
      },
      "source": [
        "# 5.NotAI-tech English-Telugu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqarCerRPSMg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4736bddf-3b2b-4bde-97a2-a48ddc6bc0a0"
      },
      "source": [
        "# NotAI-tech English-Telugu: Around 38k word pairs\n",
        "# en_te_wiki_titles: contains 13,811 word en-te pairs, generated from Wikipedia by comparing titles of parallel articles.\n",
        "!wget https://github.com/notAI-tech/Datasets/releases/download/En-Te_Transliteration/v1.en_te_wiki_titles.txt\n",
        "# ni_bondha_comments: contains 24,757 word en-te pairs.\n",
        "!wget https://github.com/notAI-tech/Datasets/releases/download/En-Te_Transliteration/v1.ni_bondha_comment_words.txt\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-16 11:08:30--  https://github.com/notAI-tech/Datasets/releases/download/En-Te_Transliteration/v1.en_te_wiki_titles.txt\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-releases.githubusercontent.com/256998536/480acb80-826b-11ea-858b-44be5a92e9ca?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210716%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210716T110830Z&X-Amz-Expires=300&X-Amz-Signature=3101a7330c90919ddf78679c7f2bf45344d2c0ca6ec5d2d7617a1864d386c4f0&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=256998536&response-content-disposition=attachment%3B%20filename%3Dv1.en_te_wiki_titles.txt&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-07-16 11:08:30--  https://github-releases.githubusercontent.com/256998536/480acb80-826b-11ea-858b-44be5a92e9ca?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210716%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210716T110830Z&X-Amz-Expires=300&X-Amz-Signature=3101a7330c90919ddf78679c7f2bf45344d2c0ca6ec5d2d7617a1864d386c4f0&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=256998536&response-content-disposition=attachment%3B%20filename%3Dv1.en_te_wiki_titles.txt&response-content-type=application%2Foctet-stream\n",
            "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.108.154, 185.199.109.154, 185.199.110.154, ...\n",
            "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.108.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 435075 (425K) [application/octet-stream]\n",
            "Saving to: ‘v1.en_te_wiki_titles.txt’\n",
            "\n",
            "v1.en_te_wiki_title 100%[===================>] 424.88K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-07-16 11:08:31 (3.15 MB/s) - ‘v1.en_te_wiki_titles.txt’ saved [435075/435075]\n",
            "\n",
            "--2021-07-16 11:08:31--  https://github.com/notAI-tech/Datasets/releases/download/En-Te_Transliteration/v1.ni_bondha_comment_words.txt\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-releases.githubusercontent.com/256998536/2b739100-8275-11ea-9a76-3bfcd6ac0bd4?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210716%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210716T110831Z&X-Amz-Expires=300&X-Amz-Signature=15c5f1d38a5705bd81db8086c3b603cd72745368d31792c7ac9c90887fc70898&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=256998536&response-content-disposition=attachment%3B%20filename%3Dv1.ni_bondha_comment_words.txt&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-07-16 11:08:31--  https://github-releases.githubusercontent.com/256998536/2b739100-8275-11ea-9a76-3bfcd6ac0bd4?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210716%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210716T110831Z&X-Amz-Expires=300&X-Amz-Signature=15c5f1d38a5705bd81db8086c3b603cd72745368d31792c7ac9c90887fc70898&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=256998536&response-content-disposition=attachment%3B%20filename%3Dv1.ni_bondha_comment_words.txt&response-content-type=application%2Foctet-stream\n",
            "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.108.154, 185.199.109.154, 185.199.110.154, ...\n",
            "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.108.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 808542 (790K) [application/octet-stream]\n",
            "Saving to: ‘v1.ni_bondha_comment_words.txt’\n",
            "\n",
            "v1.ni_bondha_commen 100%[===================>] 789.59K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-07-16 11:08:31 (15.9 MB/s) - ‘v1.ni_bondha_comment_words.txt’ saved [808542/808542]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4URCbgQ98Ti4"
      },
      "source": [
        "import re\n",
        "dataset = \"NotAI-tech English-Telugu\"\n",
        "lang = \"Telugu\"\n",
        "lang_abr = 'te'\n",
        "file_type = 'train'\n",
        "\n",
        "\n",
        "f_in = open('/content/v1.en_te_wiki_titles.txt','r')\n",
        "\n",
        "f_out = open('Indic_Xlit_transliteration_parallel_corpora/'+lang+'/' + lang_abr + '_'+file_type+'.txt', \"a\")\n",
        "\n",
        "lines = f_in.read().split(\"\\n\")\n",
        "\n",
        "# taking first 2 word and joining with \\t\n",
        "lines = [ \"\\t\".join( line.split(\"\\t\")[::-1] ) for line in lines ]\n",
        "\n",
        "# removing empty strings\n",
        "data_lines = [line for line in lines if '\\t' in line]\n",
        "\n",
        "# removing duplicates\n",
        "data_lines = list(set(data_lines))\n",
        "\n",
        "count = len(data_lines)\n",
        "\n",
        "f_out.write('\\n'.join(data_lines))\n",
        "\n",
        "f_out.close()\n",
        "f_in.close()\n",
        "\n",
        "\n",
        "f_in = open('/content/v1.ni_bondha_comment_words.txt','r')\n",
        "\n",
        "f_out = open('Indic_Xlit_transliteration_parallel_corpora/'+lang+'/' + lang_abr + '_'+file_type+'.txt', \"a\")\n",
        "\n",
        "lines = f_in.read().split(\"\\n\")\n",
        "\n",
        "# taking first 2 word and joining with \\t\n",
        "lines = [ \"\\t\".join( line.split(\"\\t\")[::-1] ) for line in lines ]\n",
        "\n",
        "# removing special characters and emojis\n",
        "pattern = '[!@#$\\\")(\\'\\,%^&*?+:;{}<>/|\\[\\].`~-]'\n",
        "lines = [re.sub(pattern, '', line) for line in lines]\n",
        "\n",
        "\n",
        "regrex_pattern = re.compile(pattern = \"[\"\n",
        "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                        \"]+\", flags = re.UNICODE)\n",
        "\n",
        "lines = [regrex_pattern.sub(r'',line) for line in lines]\n",
        "\n",
        "# removing empty strings\n",
        "data_lines = [line for line in lines if '\\t' in line]\n",
        "\n",
        "# removing duplicates\n",
        "data_lines = list(set(data_lines))\n",
        "\n",
        "count += len(data_lines) \n",
        "\n",
        "f_out.write('\\n'.join(data_lines))\n",
        "\n",
        "f_out.close()\n",
        "f_in.close()\n",
        "\n",
        "# counting word_pairs\n",
        "word_pairs_count[lang_index[lang]][dataset_index[dataset]][sampling_index[file_type]] = count\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_0rkOZdY1yu"
      },
      "source": [
        "# 6.Brahminet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc_oZ-rPY8qX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a11d9fc-17e0-4b84-deb9-c93f295d83d7"
      },
      "source": [
        "# Brahminet\n",
        "!wget https://akpublicdata.blob.core.windows.net/publicdata/transliteration_data/brahminet.zip\n",
        "!unzip brahminet.zip"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-16 11:08:32--  https://akpublicdata.blob.core.windows.net/publicdata/transliteration_data/brahminet.zip\n",
            "Resolving akpublicdata.blob.core.windows.net (akpublicdata.blob.core.windows.net)... 52.239.246.4\n",
            "Connecting to akpublicdata.blob.core.windows.net (akpublicdata.blob.core.windows.net)|52.239.246.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23490996 (22M) [application/x-zip-compressed]\n",
            "Saving to: ‘brahminet.zip’\n",
            "\n",
            "brahminet.zip       100%[===================>]  22.40M  15.4MB/s    in 1.5s    \n",
            "\n",
            "2021-07-16 11:08:34 (15.4 MB/s) - ‘brahminet.zip’ saved [23490996/23490996]\n",
            "\n",
            "Archive:  brahminet.zip\n",
            "  inflating: brahminet/bn-en.csv     \n",
            "  inflating: brahminet/bn-gu.csv     \n",
            "  inflating: brahminet/bn-hi.csv     \n",
            "  inflating: brahminet/bn-kK.csv     \n",
            "  inflating: brahminet/bn-ml.csv     \n",
            "  inflating: brahminet/bn-mr.csv     \n",
            "  inflating: brahminet/bn-pa.csv     \n",
            "  inflating: brahminet/bn-ta.csv     \n",
            "  inflating: brahminet/bn-te.csv     \n",
            "  inflating: brahminet/bn-ur.csv     \n",
            "  inflating: brahminet/brahminet.acronyms.en  \n",
            "  inflating: brahminet/brahminet.vocab.en  \n",
            "  inflating: brahminet/en-bn.csv     \n",
            "  inflating: brahminet/en-gu.csv     \n",
            "  inflating: brahminet/en-hi.csv     \n",
            "  inflating: brahminet/en-kK.csv     \n",
            "  inflating: brahminet/en-ml.csv     \n",
            "  inflating: brahminet/en-mr.csv     \n",
            "  inflating: brahminet/en-pa.csv     \n",
            "  inflating: brahminet/en-ta.csv     \n",
            "  inflating: brahminet/en-te.csv     \n",
            "  inflating: brahminet/en-ur.csv     \n",
            "  inflating: brahminet/gu-bn.csv     \n",
            "  inflating: brahminet/gu-en.csv     \n",
            "  inflating: brahminet/gu-hi.csv     \n",
            "  inflating: brahminet/gu-kK.csv     \n",
            "  inflating: brahminet/gu-ml.csv     \n",
            "  inflating: brahminet/gu-mr.csv     \n",
            "  inflating: brahminet/gu-pa.csv     \n",
            "  inflating: brahminet/gu-ta.csv     \n",
            "  inflating: brahminet/gu-te.csv     \n",
            "  inflating: brahminet/gu-ur.csv     \n",
            "  inflating: brahminet/hi-bn.csv     \n",
            "  inflating: brahminet/hi-en.csv     \n",
            "  inflating: brahminet/hi-gu.csv     \n",
            "  inflating: brahminet/hi-kK.csv     \n",
            "  inflating: brahminet/hi-ml.csv     \n",
            "  inflating: brahminet/hi-mr.csv     \n",
            "  inflating: brahminet/hi-pa.csv     \n",
            "  inflating: brahminet/hi-ta.csv     \n",
            "  inflating: brahminet/hi-te.csv     \n",
            "  inflating: brahminet/hi-ur.csv     \n",
            "  inflating: brahminet/kK-bn.csv     \n",
            "  inflating: brahminet/kK-en.csv     \n",
            "  inflating: brahminet/kK-gu.csv     \n",
            "  inflating: brahminet/kK-hi.csv     \n",
            "  inflating: brahminet/kK-ml.csv     \n",
            "  inflating: brahminet/kK-mr.csv     \n",
            "  inflating: brahminet/kK-pa.csv     \n",
            "  inflating: brahminet/kK-ta.csv     \n",
            "  inflating: brahminet/kK-te.csv     \n",
            "  inflating: brahminet/kK-ur.csv     \n",
            "  inflating: brahminet/ml-bn.csv     \n",
            "  inflating: brahminet/ml-en.csv     \n",
            "  inflating: brahminet/ml-gu.csv     \n",
            "  inflating: brahminet/ml-hi.csv     \n",
            "  inflating: brahminet/ml-kK.csv     \n",
            "  inflating: brahminet/ml-mr.csv     \n",
            "  inflating: brahminet/ml-pa.csv     \n",
            "  inflating: brahminet/ml-ta.csv     \n",
            "  inflating: brahminet/ml-te.csv     \n",
            "  inflating: brahminet/ml-ur.csv     \n",
            "  inflating: brahminet/mr-bn.csv     \n",
            "  inflating: brahminet/mr-en.csv     \n",
            "  inflating: brahminet/mr-gu.csv     \n",
            "  inflating: brahminet/mr-hi.csv     \n",
            "  inflating: brahminet/mr-kK.csv     \n",
            "  inflating: brahminet/mr-ml.csv     \n",
            "  inflating: brahminet/mr-pa.csv     \n",
            "  inflating: brahminet/mr-ta.csv     \n",
            "  inflating: brahminet/mr-te.csv     \n",
            "  inflating: brahminet/mr-ur.csv     \n",
            "  inflating: brahminet/pa-bn.csv     \n",
            "  inflating: brahminet/pa-en.csv     \n",
            "  inflating: brahminet/pa-gu.csv     \n",
            "  inflating: brahminet/pa-hi.csv     \n",
            "  inflating: brahminet/pa-kK.csv     \n",
            "  inflating: brahminet/pa-ml.csv     \n",
            "  inflating: brahminet/pa-mr.csv     \n",
            "  inflating: brahminet/pa-ta.csv     \n",
            "  inflating: brahminet/pa-te.csv     \n",
            "  inflating: brahminet/pa-ur.csv     \n",
            "  inflating: brahminet/ta-bn.csv     \n",
            "  inflating: brahminet/ta-en.csv     \n",
            "  inflating: brahminet/ta-gu.csv     \n",
            "  inflating: brahminet/ta-hi.csv     \n",
            "  inflating: brahminet/ta-kK.csv     \n",
            "  inflating: brahminet/ta-ml.csv     \n",
            "  inflating: brahminet/ta-mr.csv     \n",
            "  inflating: brahminet/ta-pa.csv     \n",
            "  inflating: brahminet/ta-te.csv     \n",
            "  inflating: brahminet/ta-ur.csv     \n",
            "  inflating: brahminet/te-bn.csv     \n",
            "  inflating: brahminet/te-en.csv     \n",
            "  inflating: brahminet/te-gu.csv     \n",
            "  inflating: brahminet/te-hi.csv     \n",
            "  inflating: brahminet/te-kK.csv     \n",
            "  inflating: brahminet/te-ml.csv     \n",
            "  inflating: brahminet/te-mr.csv     \n",
            "  inflating: brahminet/te-pa.csv     \n",
            "  inflating: brahminet/te-ta.csv     \n",
            "  inflating: brahminet/te-ur.csv     \n",
            "  inflating: brahminet/ur-bn.csv     \n",
            "  inflating: brahminet/ur-en.csv     \n",
            "  inflating: brahminet/ur-gu.csv     \n",
            "  inflating: brahminet/ur-hi.csv     \n",
            "  inflating: brahminet/ur-kK.csv     \n",
            "  inflating: brahminet/ur-ml.csv     \n",
            "  inflating: brahminet/ur-mr.csv     \n",
            "  inflating: brahminet/ur-pa.csv     \n",
            "  inflating: brahminet/ur-ta.csv     \n",
            "  inflating: brahminet/ur-te.csv     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNFQZN7Hn9pU"
      },
      "source": [
        "lang_list = ['Bangla', 'Gujarati', 'Hindi', 'Malayalam', 'Marathi', 'Punjabi', 'Tamil', 'Telugu', 'Urdu', 'Konkani']\n",
        "lang_abr_in_list = ['bn', 'gu', 'hi', 'ml', 'mr', 'pa', 'ta', 'te', 'ur', 'kK']\n",
        "lang_abr_out_list = ['bn', 'gu', 'hi', 'ml', 'mr', 'pa', 'ta', 'te', 'ur', 'gom']\n",
        "file_out_type = \"train\"\n",
        "\n",
        "dataset = \"Brahminet\"\n",
        "\n",
        "for lang, lang_abr_in, lang_abr_out  in zip(lang_list, lang_abr_in_list, lang_abr_out_list):\n",
        "    \n",
        "    count = 0\n",
        "    \n",
        "    # Opening JSON file\n",
        "    f_in_1 = open('/content/brahminet/en-'+lang_abr_in+'.csv','r')\n",
        "    \n",
        "    \n",
        "    f_in_1_1ines = f_in_1.read().split('\\n')\n",
        "    f_in_1_1ines = ['\\t'.join(line.split('|')[:2][::-1]) for line in f_in_1_1ines]\n",
        "    f_in_1_1ines = [line for line in f_in_1_1ines if line]\n",
        "    \n",
        "    # removing duplicates from \n",
        "    f_in_1_1ines_set = set(f_in_1_1ines)\n",
        "\n",
        "    # counting word pairs\n",
        "    count = len(list(f_in_1_1ines_set))\n",
        "\n",
        "    # write en-native file\n",
        "    f_out = open('Indic_Xlit_transliteration_parallel_corpora/'+lang+'/' + lang_abr_out + '_'+file_out_type+'.txt', \"a\")\n",
        "    f_out.write(\"\\n\".join(list(f_in_1_1ines_set)))\n",
        "    f_out.write(\"\\n\")\n",
        "    f_out.close()\n",
        "\n",
        "    f_in_2 = open('/content/brahminet/'+lang_abr_in+'-en.csv','r')\n",
        "    f_in_2_1ines = f_in_2.read().split('\\n')\n",
        "    f_in_2_1ines = ['\\t'.join(line.split('|')[:2] ) for line in f_in_2_1ines]\n",
        "    f_in_2_1ines = [line for line in f_in_2_1ines if line]\n",
        "\n",
        "\n",
        "    # removing duplicates\n",
        "    f_in_2_1ines_set = set(f_in_2_1ines)\n",
        "\n",
        "    # checking if there any new word pairs in the reverse file.\n",
        "    diff_set = f_in_1_1ines_set.difference(f_in_2_1ines_set)\n",
        "\n",
        "    count += len(list(diff_set))\n",
        "\n",
        "    # write en-native file\n",
        "    f_out = open('Indic_Xlit_transliteration_parallel_corpora/'+lang+'/' + lang_abr_out + '_'+file_out_type+'.txt', \"a\")\n",
        "    f_out.write(\"\\n\".join(list(diff_set)))\n",
        "    f_out.write(\"\\n\")\n",
        "    f_out.close()\n",
        "    f_in_1.close()\n",
        "    f_in_2.close()\n",
        "\n",
        "    # counting word_pairs\n",
        "    word_pairs_count[lang_index[lang]][dataset_index[dataset]][sampling_index[file_out_type]] = count\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mNG-4hhTeRC"
      },
      "source": [
        "# 7.NEWS 2018 Shared Task dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQlAqrg_NuzB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec0a2a58-415c-40df-e235-6a0c4822e722"
      },
      "source": [
        "# NEWS 2018 Shared Task dataset: Transliteration datasets for Kannada, Tamil, Bengali and Hindi created by Microsoft Research India.\n",
        "!wget http://workshop.colips.org/news2018/ihfggiehbf4hahbfhhf/NEWS2018_DATASET_04.zip\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile('/content/NEWS2018_DATASET_04.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')\n",
        "\n",
        "# have to translate xml to txt\n",
        "\n",
        "# eng-hindi test data\n",
        "!wget http://workshop.colips.org/news2018/testdata2018/NEWS2018_M-EnHi_tst.zip\n",
        "\n",
        "with zipfile.ZipFile('/content/NEWS2018_M-EnHi_tst.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')\n",
        "\n",
        "# eng-tamil test data\n",
        "!wget http://workshop.colips.org/news2018/testdata2018/NEWS2018_M-EnTa_tst.zip\n",
        "with zipfile.ZipFile('/content/NEWS2018_M-EnTa_tst.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')\n",
        "\n",
        "# eng-kannada test data\n",
        "!wget http://workshop.colips.org/news2018/testdata2018/NEWS2018_M-EnKa_tst.zip\n",
        "with zipfile.ZipFile('/content/NEWS2018_M-EnKa_tst.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')\n",
        "\n",
        "# eng-bangala test data\n",
        "!wget http://workshop.colips.org/news2018/testdata2018/NEWS2018_M-EnBa_tst.zip\n",
        "with zipfile.ZipFile('/content/NEWS2018_M-EnBa_tst.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-16 11:08:35--  http://workshop.colips.org/news2018/ihfggiehbf4hahbfhhf/NEWS2018_DATASET_04.zip\n",
            "Resolving workshop.colips.org (workshop.colips.org)... 139.59.194.7\n",
            "Connecting to workshop.colips.org (workshop.colips.org)|139.59.194.7|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1171714 (1.1M) [application/zip]\n",
            "Saving to: ‘NEWS2018_DATASET_04.zip’\n",
            "\n",
            "NEWS2018_DATASET_04 100%[===================>]   1.12M   963KB/s    in 1.2s    \n",
            "\n",
            "2021-07-16 11:08:37 (963 KB/s) - ‘NEWS2018_DATASET_04.zip’ saved [1171714/1171714]\n",
            "\n",
            "--2021-07-16 11:08:37--  http://workshop.colips.org/news2018/testdata2018/NEWS2018_M-EnHi_tst.zip\n",
            "Resolving workshop.colips.org (workshop.colips.org)... 139.59.194.7\n",
            "Connecting to workshop.colips.org (workshop.colips.org)|139.59.194.7|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8213 (8.0K) [application/zip]\n",
            "Saving to: ‘NEWS2018_M-EnHi_tst.zip’\n",
            "\n",
            "NEWS2018_M-EnHi_tst 100%[===================>]   8.02K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-07-16 11:08:37 (177 MB/s) - ‘NEWS2018_M-EnHi_tst.zip’ saved [8213/8213]\n",
            "\n",
            "--2021-07-16 11:08:37--  http://workshop.colips.org/news2018/testdata2018/NEWS2018_M-EnTa_tst.zip\n",
            "Resolving workshop.colips.org (workshop.colips.org)... 139.59.194.7\n",
            "Connecting to workshop.colips.org (workshop.colips.org)|139.59.194.7|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8427 (8.2K) [application/zip]\n",
            "Saving to: ‘NEWS2018_M-EnTa_tst.zip’\n",
            "\n",
            "NEWS2018_M-EnTa_tst 100%[===================>]   8.23K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-07-16 11:08:37 (152 MB/s) - ‘NEWS2018_M-EnTa_tst.zip’ saved [8427/8427]\n",
            "\n",
            "--2021-07-16 11:08:37--  http://workshop.colips.org/news2018/testdata2018/NEWS2018_M-EnKa_tst.zip\n",
            "Resolving workshop.colips.org (workshop.colips.org)... 139.59.194.7\n",
            "Connecting to workshop.colips.org (workshop.colips.org)|139.59.194.7|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8394 (8.2K) [application/zip]\n",
            "Saving to: ‘NEWS2018_M-EnKa_tst.zip’\n",
            "\n",
            "NEWS2018_M-EnKa_tst 100%[===================>]   8.20K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-07-16 11:08:38 (157 MB/s) - ‘NEWS2018_M-EnKa_tst.zip’ saved [8394/8394]\n",
            "\n",
            "--2021-07-16 11:08:38--  http://workshop.colips.org/news2018/testdata2018/NEWS2018_M-EnBa_tst.zip\n",
            "Resolving workshop.colips.org (workshop.colips.org)... 139.59.194.7\n",
            "Connecting to workshop.colips.org (workshop.colips.org)|139.59.194.7|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8732 (8.5K) [application/zip]\n",
            "Saving to: ‘NEWS2018_M-EnBa_tst.zip’\n",
            "\n",
            "NEWS2018_M-EnBa_tst 100%[===================>]   8.53K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-07-16 11:08:38 (132 MB/s) - ‘NEWS2018_M-EnBa_tst.zip’ saved [8732/8732]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIXrzERScnZ9"
      },
      "source": [
        "# adding dev data\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "dataset= \"NEWS 2018 Shared Task dataset\"\n",
        "lang_list = [\"Kannada\", \"Tamil\", \"Bangla\", \"Hindi\"]\n",
        "lang_abr_in_list = ['Ka', 'Ta', 'Ba', 'Hi']\n",
        "lang_abr_out_list = ['kn', 'ta', 'bn', 'hi']\n",
        "sampling_in = ['dev']\n",
        "sampling_out = ['dev']\n",
        "\n",
        "\n",
        "\n",
        "for lang, lang_abr_in, lang_abr_out  in zip(lang_list, lang_abr_in_list, lang_abr_out_list):\n",
        "    for file_in_type, file_out_type in zip(sampling_in, sampling_out):\n",
        "\n",
        "        tree = ET.parse('/content/NEWS2018_DATASET_04/NEWS2018_M-En'+lang_abr_in+'_'+file_in_type+'.xml')\n",
        "        root = tree.getroot()\n",
        "\n",
        "        temp_list = []\n",
        "        for word_pairs in root:\n",
        "            for element in word_pairs:\n",
        "                if element.tag == \"SourceName\":\n",
        "                    rom = element.text \n",
        "                elif element.tag == \"TargetName\":\n",
        "                    nat = element.text\n",
        "                    temp_list.append(nat+\"\\t\"+rom)  \n",
        "\n",
        "        lines = temp_list\n",
        "    \n",
        "        # removing duplicates\n",
        "        lines = list(set(lines))\n",
        "\n",
        "        f_out = open('Indic_Xlit_transliteration_parallel_corpora/'+lang+'/' + lang_abr_out + '_'+file_out_type+'.txt', \"a\")\n",
        "\n",
        "        count = 0\n",
        "        for line in lines:\n",
        "            if \" \" in line:\n",
        "                \n",
        "                sentences = line.split('\\t')\n",
        "                nat_sen = sentences[0]\n",
        "                rom_sen = sentences[1]\n",
        "                nat_words = nat_sen.split(' ')\n",
        "                rom_words = rom_sen.split(' ')\n",
        "                \n",
        "                for nat, rom in zip(nat_words, rom_words):\n",
        "                    f_out.write(nat+\"\\t\"+rom+\"\\n\")\n",
        "                    count+=1\n",
        "            else:\n",
        "                f_out.write(line+\"\\n\")\n",
        "                count+=1\n",
        "        \n",
        "        # counting word_pairs\n",
        "        word_pairs_count[lang_index[lang]][dataset_index[dataset]][sampling_index[file_out_type]] = count\n",
        "\n",
        "        \n",
        "        f_out.close()\n",
        "        f_in.close()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VEpn7lnnfQF"
      },
      "source": [
        "# import xml.etree.ElementTree as ET\n",
        "\n",
        "# # adding test data\n",
        "# dataset = \"NEWS 2018 Shared Task dataset\"\n",
        "# lang_list = [\"Kannada\", \"Tamil\", \"Bangla\", \"Hindi\"]\n",
        "# lang_abr_in_list = ['Ka', 'Ta', 'Ba', 'Hi']\n",
        "# lang_abr_out_list = ['kn', 'ta', 'bn', 'hi']\n",
        "# file_in_type = 'tst'\n",
        "# file_out_type = 'test'\n",
        "\n",
        "# for lang, lang_abr_in, lang_abr_out  in zip(lang_list, lang_abr_in_list, lang_abr_out_list):\n",
        "\n",
        "#     tree = ET.parse('/content/NEWS2018_M-En'+lang_abr_in+'_'+file_in_type+'.xml')\n",
        "#     root = tree.getroot()\n",
        "\n",
        "#     temp_list = []\n",
        "#     for word_pairs in root:\n",
        "#         for element in word_pairs:\n",
        "#             if element.tag == \"SourceName\":\n",
        "#                 rom = element.text \n",
        "#             elif element.tag == \"TargetName\":\n",
        "#                 nat = element.text\n",
        "#                 temp_list.append(nat+\"\\t\"+rom)  \n",
        "\n",
        "#     lines = temp_list\n",
        "#     print(root)\n",
        "#     # removing duplicates\n",
        "#     lines = list(set(lines))\n",
        "    \n",
        "#     f_out = open('Indic_Xlit_transliteration_parallel_corpora/'+lang+'/' + lang_abr_out + '_'+file_out_type+'.txt', \"a\")\n",
        "\n",
        "#     count = 0\n",
        "#     for line in lines:\n",
        "#         if \" \" in line:\n",
        "            \n",
        "#             sentences = line.split('\\t')\n",
        "#             nat_sen = sentences[0]\n",
        "#             rom_sen = sentences[1]\n",
        "#             nat_words = nat_sen.split(' ')\n",
        "#             rom_words = rom_sen.split(' ')\n",
        "            \n",
        "#             for nat, rom in zip(nat_words, rom_words):\n",
        "#                 f_out.write(nat+\"\\t\"+rom+\"\\n\")\n",
        "#                 count+=1\n",
        "#         else:\n",
        "#             f_out.write(line+\"\\n\")\n",
        "#             count+=1\n",
        "#     print(count)\n",
        "#     # counting word_pairs\n",
        "#     word_pairs_count[lang_index[lang]][dataset_index[dataset]][sampling_index[file_out_type]] = count\n",
        "\n",
        "#     f_out.close()\n",
        "#     f_in.close()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNs9tSOrtca-"
      },
      "source": [
        "# Creating csv file for word pairs count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiEkuEZZv6MA"
      },
      "source": [
        "lang_list = ['Bangla', 'Gujarati', 'Hindi', 'Kannada', 'Malayalam', 'Marathi', 'Punjabi', 'Sindhi', 'Sinhala', 'Tamil', 'Telugu', 'Urdu', 'Konkani', 'Maithili']\n",
        "lang_abr_list = ['bn', 'gu', 'hi', 'kn', 'ml', 'mr', 'pa', 'sd', 'si', 'ta', 'te', 'ur', 'gom', 'mai']\n",
        "lang_abr_dict = dict(zip(lang_list,lang_abr_list ))\n",
        "\n",
        "import csv\n",
        "\n",
        "header = list(dataset_index.keys())\n",
        "header.insert(0, \" \")\n",
        "header = header + [\"total_word_pairs\", \"total_unique_word_pairs\", \"total_unique_native_word_pairs\"]\n",
        "\n",
        "# indic-Xlit transliteration corpora\n",
        "\n",
        "import os \n",
        "os.mkdir('Indic_Xlit_translit_parallel_corpus')\n",
        "\n",
        "# creating subdirectories and corresponding txt file\n",
        "for lang, lang_abr in zip(lang_list, lang_abr_list):\n",
        "    os.mkdir('Indic_Xlit_translit_parallel_corpus/'+lang)\n",
        "    f = open('Indic_Xlit_translit_parallel_corpus/'+lang+'/' + lang_abr + '_train.txt', \"a\")\n",
        "    f.close()\n",
        "    f = open('Indic_Xlit_translit_parallel_corpus/'+lang+'/' + lang_abr + '_dev.txt', \"a\")\n",
        "    f.close()\n",
        "    f = open('Indic_Xlit_translit_parallel_corpus/'+lang+'/' + lang_abr + '_test.txt', \"a\")\n",
        "    f.close()\n",
        "\n",
        "with open('dataset_info.csv', 'w', encoding='UTF8', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "\n",
        "    for file_type in sampling_index.keys():\n",
        "        \n",
        "        header[0] = file_type+\"_data\"\n",
        "\n",
        "        # write the header\n",
        "        writer.writerow(header)\n",
        "\n",
        "        for lang in lang_index.keys():\n",
        "\n",
        "            data_train = word_pairs_count[lang_index[lang], : , sampling_index[file_type]].tolist()\n",
        "            \n",
        "            f_in = open('Indic_Xlit_transliteration_parallel_corpora/'+lang+'/' + lang_abr_dict[lang] + '_'+file_type+'.txt', \"r\")\n",
        "            lines = f_in.read().split(\"\\n\")\n",
        "            lines = [line for line in lines if '\\t' in line]\n",
        "            total_pairs = len(lines)\n",
        "            \n",
        "            \n",
        "            lines = list(set(lines))\n",
        "            \n",
        "            f_out = open('Indic_Xlit_translit_parallel_corpus/'+lang+'/' + lang_abr_dict[lang] + '_'+file_type+'.txt', \"a\")\n",
        "            f_out.write('\\n'.join(lines))\n",
        "\n",
        "            total_unique_pairs = len(lines)\n",
        "\n",
        "            native_words = [line.split('\\t')[0] for line in lines]\n",
        "            native_words = list(set(native_words))\n",
        "            total_native_unique_pairs = len(native_words)\n",
        "\n",
        "            f_in.close()\n",
        "            f_out.close()\n",
        "\n",
        "            # write multiple rows\n",
        "            writer.writerow( [lang] + data_train + [total_pairs, total_unique_pairs, total_native_unique_pairs])\n",
        "        \n",
        "        writer.writerow([])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HdMqw8JZijs"
      },
      "source": [
        "# Zip final dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhjQpzDEZRec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b95f36b0-b7f0-4fa7-bcd8-a20eb5c0cd8a"
      },
      "source": [
        "!zip -r /content/Indic_Xlit_translit_parallel_corpus.zip /content/Indic_Xlit_translit_parallel_corpus"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/Indic_Xlit_translit_parallel_corpus/ (stored 0%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Konkani/ (stored 0%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Konkani/gom_dev.txt (deflated 70%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Konkani/gom_train.txt (deflated 65%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Konkani/gom_test.txt (deflated 69%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Bangla/ (stored 0%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Bangla/bn_dev.txt (deflated 67%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Bangla/bn_train.txt (deflated 64%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Bangla/bn_test.txt (deflated 69%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Sinhala/ (stored 0%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Sinhala/si_dev.txt (deflated 69%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Sinhala/si_train.txt (deflated 66%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Sinhala/si_test.txt (deflated 69%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Marathi/ (stored 0%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Marathi/mr_train.txt (deflated 65%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Marathi/mr_dev.txt (deflated 70%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Marathi/mr_test.txt (deflated 69%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Hindi/ (stored 0%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Hindi/hi_test.txt (deflated 65%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Hindi/hi_train.txt (deflated 61%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Hindi/hi_dev.txt (deflated 64%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Tamil/ (stored 0%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Tamil/ta_train.txt (deflated 70%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Tamil/ta_test.txt (deflated 73%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Tamil/ta_dev.txt (deflated 71%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Urdu/ (stored 0%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Urdu/ur_dev.txt (deflated 60%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Urdu/ur_train.txt (deflated 54%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Urdu/ur_test.txt (deflated 60%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Sindhi/ (stored 0%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Sindhi/sd_test.txt (deflated 60%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Sindhi/sd_train.txt (deflated 55%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Sindhi/sd_dev.txt (deflated 60%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Maithili/ (stored 0%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Maithili/mai_test.txt (deflated 65%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Maithili/mai_dev.txt (deflated 67%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Maithili/mai_train.txt (deflated 62%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Punjabi/ (stored 0%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Punjabi/pa_dev.txt (deflated 66%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Punjabi/pa_train.txt (deflated 62%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Punjabi/pa_test.txt (deflated 66%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Gujarati/ (stored 0%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Gujarati/gu_train.txt (deflated 64%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Gujarati/gu_test.txt (deflated 69%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Gujarati/gu_dev.txt (deflated 69%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Kannada/ (stored 0%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Kannada/kn_test.txt (deflated 72%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Kannada/kn_train.txt (deflated 69%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Kannada/kn_dev.txt (deflated 69%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Telugu/ (stored 0%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Telugu/te_test.txt (deflated 70%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Telugu/te_dev.txt (deflated 69%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Telugu/te_train.txt (deflated 65%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Malayalam/ (stored 0%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Malayalam/ml_dev.txt (deflated 72%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Malayalam/ml_test.txt (deflated 72%)\n",
            "  adding: content/Indic_Xlit_translit_parallel_corpus/Malayalam/ml_train.txt (deflated 68%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}